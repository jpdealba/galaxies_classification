# IntroducciÃ³n al Aprendizaje AutomÃ¡tico: ClasificaciÃ³n Binaria de Estrellas y Galaxias

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![Jupyter](https://img.shields.io/badge/jupyter-notebook-orange.svg)](https://jupyter.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto desarrolla un sistema de clasificaciÃ³n automÃ¡tica para distinguir entre **estrellas** y **galaxias** utilizando tÃ©cnicas de aprendizaje automÃ¡tico aplicadas a datos fotomÃ©tricos del **Sloan Digital Sky Survey (SDSS)**. 

La astronomÃ­a moderna enfrenta el desafÃ­o de procesar enormes volÃºmenes de datos provenientes de observaciones astronÃ³micas. Con el advenimiento de telescopios de gran campo, la cantidad de objetos celestes detectados ha crecido exponencialmente, haciendo impracticable la clasificaciÃ³n manual. Este proyecto aborda este problema implementando y comparando mÃºltiples algoritmos de machine learning.

## ğŸ¯ Objetivos

### Objetivo Principal
Desarrollar un sistema de clasificaciÃ³n automÃ¡tica que permita distinguir eficientemente entre estrellas y galaxias utilizando tÃ©cnicas de aprendizaje automÃ¡tico.

### Objetivos EspecÃ­ficos
- Implementar y comparar el rendimiento de **4 algoritmos** de clasificaciÃ³n binaria:
  - Random Forest
  - RegresiÃ³n LogÃ­stica  
  - MÃ¡quinas de Vectores de Soporte (SVM)
  - K-Nearest Neighbors (KNN)
- Realizar anÃ¡lisis exhaustivo de datos fotomÃ©tricos
- Aplicar tÃ©cnicas de preprocesamiento optimizadas
- Optimizar hiperparÃ¡metros mediante bÃºsqueda aleatoria
- Alcanzar precisiÃ³n superior al **90%** en clasificaciÃ³n

## ğŸ“Š Dataset

### Fuente de Datos
- **Origen**: Sloan Digital Sky Survey (SDSS)
- **TamaÃ±o total**: ~5 millones de observaciones
  - **Entrenamiento**: 4 millones (2M estrellas + 2M galaxias)
  - **Prueba**: 1 millÃ³n de observaciones
- **Variables**: 50 caracterÃ­sticas observacionales

### Estructura de Datos
```
â”œâ”€â”€ Identificadores y Metadatos
â”‚   â”œâ”€â”€ objID, run, camcol, field
â”œâ”€â”€ PosiciÃ³n y Movimiento  
â”‚   â”œâ”€â”€ ra, dec (coordenadas ecuatoriales)
â”‚   â”œâ”€â”€ b, l (coordenadas galÃ¡cticas)
â”‚   â””â”€â”€ rowv, colv (velocidades)
â”œâ”€â”€ Magnitudes FotomÃ©tricas
â”‚   â”œâ”€â”€ psfMag_* (magnitudes PSF en 5 filtros)
â”‚   â”œâ”€â”€ u, g, r, i, z (magnitudes modelo)
â”‚   â””â”€â”€ modelFlux_* (flujos correspondientes)
â”œâ”€â”€ ParÃ¡metros MorfolÃ³gicos
â”‚   â”œâ”€â”€ petroRad_* (radios petrosianos)
â”‚   â”œâ”€â”€ expRad_* (radios exponenciales)
â”‚   â””â”€â”€ expAB_* (relaciones de ejes)
â””â”€â”€ ParÃ¡metros de Stokes
    â”œâ”€â”€ q_* (polarizaciÃ³n Q)
    â””â”€â”€ u_* (polarizaciÃ³n U)
```

## ğŸ”§ MetodologÃ­a

### 1. ExploraciÃ³n de Datos
- **AnÃ¡lisis de calidad**: VerificaciÃ³n de valores faltantes, duplicados
- **EstadÃ­sticas descriptivas**: Medidas de tendencia central y dispersiÃ³n
- **Distribuciones**: AnÃ¡lisis de histogramas y detecciÃ³n de outliers
- **Correlaciones**: Matriz completa y correlaciÃ³n con variable objetivo

### 2. Preprocesamiento
```python
# Pipeline de preprocesamiento
â”œâ”€â”€ Limpieza de datos
â”‚   â”œâ”€â”€ EliminaciÃ³n de identificadores no informativos
â”‚   â”œâ”€â”€ RemociÃ³n de variables con alta proporciÃ³n de ceros
â”‚   â””â”€â”€ CodificaciÃ³n de variable objetivo (star=0, galaxy=1)
â”œâ”€â”€ SelecciÃ³n de caracterÃ­sticas (13 variables finales)
â”‚   â”œâ”€â”€ expRad_* (radios exponenciales)
â”‚   â”œâ”€â”€ petroRad_* (radios petrosianos)  
â”‚   â””â”€â”€ g, r, i, z (magnitudes fotomÃ©tricas)
â””â”€â”€ Transformaciones
    â”œâ”€â”€ TransformaciÃ³n logarÃ­tmica (log1p)
    â”œâ”€â”€ ImputaciÃ³n con mediana
    â””â”€â”€ Escalado robusto (RobustScaler)
```

### 3. Modelado
- **Algoritmos implementados**: Random Forest, Logistic Regression, SVM, KNN
- **ValidaciÃ³n**: ValidaciÃ³n cruzada estratificada (5-fold)
- **OptimizaciÃ³n**: RandomizedSearchCV para hiperparÃ¡metros
- **MÃ©tricas**: Accuracy, F1-Score, Recall, Precision

## ğŸ“ˆ Resultados Principales

### ComparaciÃ³n de Modelos

| Modelo | Accuracy (CV) | F1-Score | AnÃ¡lisis de Overfitting |
|--------|---------------|----------|-------------------------|
| **Random Forest** | **95.71%** | 0.9571 | âš ï¸ Overfitting severo (0% vs 4.29%) |
| **Logistic Regression** | 94.30% | 0.9430 | âœ… Excelente generalizaciÃ³n (5.72% vs 5.70%) |
| **SVM** | 95.45% | 0.9545 | âš ï¸ Ligero overfitting (4.25% vs 4.55%) |
| **KNN** | 94.25% | 0.9425 | âš ï¸ Overfitting moderado (4.15% vs 5.75%) |

### Modelo Final Seleccionado: **Logistic Regression**

**JustificaciÃ³n de selecciÃ³n:**
- âœ… **Excelente capacidad de generalizaciÃ³n** (sin overfitting)
- âœ… **Eficiencia computacional** superior
- âœ… **Interpretabilidad** de coeficientes
- âœ… **Escalabilidad** para grandes datasets

### Rendimiento Final
```
ğŸ¯ Accuracy en Test Set: 94.24%
ğŸ“Š Matriz de ConfusiÃ³n:
    â”œâ”€â”€ Estrellas correctas: 942,122
    â”œâ”€â”€ Galaxias correctas: 942,614  
    â”œâ”€â”€ Falsos positivos: 57,390
    â””â”€â”€ Falsos negativos: 57,874
```

## ğŸš€ InstalaciÃ³n y Uso

### Prerrequisitos
```bash
Python 3.8+
```

### InstalaciÃ³n de Dependencias
```bash
pip install -r requirements.txt
```

### Estructura de requirements.txt
```
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.4.0
seaborn>=0.11.0
jupyter>=1.0.0
```

### Uso BÃ¡sico
```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.linear_model import LogisticRegression

# Cargar datos
df = pd.read_csv('star_classification.csv')

# CaracterÃ­sticas seleccionadas
features = [
    'expRad_r', 'expRad_i', 'expRad_g',
    'petroRad_r', 'expRad_z', 'petroRad_i',
    'petroRad_g', 'i', 'petroRad_z', 
    'expRad_u', 'z', 'r', 'g'
]

# Pipeline del modelo
pipeline = Pipeline([
    ("preprocessing", preprocessing_pipeline),
    ("classifier", LogisticRegression(
        penalty="l1",
        C=0.4655,
        l1_ratio=0.6075,
        solver="saga",
        max_iter=1000,
        random_state=42
    ))
])

# Entrenar modelo
X = df[features]
y = df['type_numeric']
pipeline.fit(X, y)

# Hacer predicciones
predictions = pipeline.predict(X_test)
```

## ğŸ“ Estructura del Proyecto

```
proyecto/
â”œâ”€â”€ documento/
â”‚   â”œâ”€â”€ main.tex                    # Documento LaTeX principal
â”‚   â”œâ”€â”€ out/
â”‚   â”‚   â””â”€â”€ main.pdf               # Documento compilado
â”‚   â””â”€â”€ imagenes/                  # Figuras y grÃ¡ficos
â”œâ”€â”€ Proyecto (3).ipynb            # Notebook principal de implementaciÃ³n
â”œâ”€â”€ matriz_correlacion_heatmap.xlsx # AnÃ¡lisis de correlaciones
â”œâ”€â”€ README.md                      # Este archivo
â””â”€â”€ requirements.txt               # Dependencias del proyecto
```

## ğŸ“Š Notebooks y Recursos

### Notebook Principal
ğŸ”— **[Google Colab - ImplementaciÃ³n Completa](https://colab.research.google.com/drive/1Z7cGOq95QmInkWO31x2LaOZ4O5ohhJIZ?usp=sharing)**

### Dataset
ğŸ”— **[Kaggle - CelestialClassify Dataset](https://www.kaggle.com/datasets/hari31416/celestialclassify)**

## ğŸ” Aspectos TÃ©cnicos Destacados

### Pipeline de Preprocesamiento Robusto
- **TransformaciÃ³n logarÃ­tmica**: Manejo de distribuciones sesgadas
- **RobustScaler**: Resistente a outliers astronÃ³micos
- **SelecciÃ³n inteligente**: 13/50 variables mÃ¡s discriminativas

### OptimizaciÃ³n de HiperparÃ¡metros
```python
# RandomizedSearchCV configuraciÃ³n
param_distributions = {
    'classifier__penalty': ['l1', 'l2', 'elasticnet'],
    'classifier__C': uniform(0.001, 10),
    'classifier__l1_ratio': uniform(0, 1)
}
```

### AnÃ¡lisis de Overfitting
- ComparaciÃ³n sistemÃ¡tica entre errores de entrenamiento y validaciÃ³n
- DetecciÃ³n de memorizaciÃ³n en modelos complejos
- PriorizaciÃ³n de capacidad de generalizaciÃ³n

## ğŸ† Logros del Proyecto

- âœ… **Accuracy superior al objetivo**: 94.24% vs objetivo de >90%
- âœ… **Dataset balanceado**: Perfect 50-50 entre estrellas y galaxias
- âœ… **Pipeline robusto**: Manejo de datos astronÃ³micos reales
- âœ… **Escalabilidad**: Procesamiento de millones de observaciones
- âœ… **MetodologÃ­a sÃ³lida**: ValidaciÃ³n cruzada estratificada

## ğŸš§ DesafÃ­os Enfrentados

### 1. **TamaÃ±o Masivo del Dataset**
- **Problema**: 4 millones de observaciones
- **SoluciÃ³n**: Muestreo estratificado para desarrollo

### 2. **Alta Dimensionalidad**
- **Problema**: 51 variables originales
- **SoluciÃ³n**: SelecciÃ³n basada en correlaciones y conocimiento del dominio

### 3. **Overfitting en Modelos Complejos**
- **Problema**: Random Forest con memorizaciÃ³n completa
- **SoluciÃ³n**: AnÃ¡lisis comparativo de errores y selecciÃ³n de modelo estable

### 4. **Distribuciones Sesgadas**
- **Problema**: Variables con distribuciones log-normales extremas
- **SoluciÃ³n**: Transformaciones logarÃ­tmicas y escalado robusto

## ğŸ’¡ Conclusiones Principales

1. **Logistic Regression** demostrÃ³ ser superior por su capacidad de generalizaciÃ³n
2. **Modelos complejos** no garantizan mejor rendimiento en datos reales
3. **Preprocesamiento robusto** es crÃ­tico para datos astronÃ³micos
4. **Balance entre eficiencia y precisiÃ³n** es clave para aplicaciones prÃ¡cticas

## ğŸ”® Aplicaciones Futuras

- **Surveys astronÃ³micos** de prÃ³xima generaciÃ³n (LSST, Euclid)
- **ClasificaciÃ³n en tiempo real** de alertas astronÃ³micas
- **ExtensiÃ³n a clasificaciÃ³n multi-clase** (diferentes tipos de galaxias)
- **IntegraciÃ³n con pipelines** de observatorios automatizados

## ğŸ‘¨â€ğŸ’» Autor

**Juan Pablo de Alba Tamayo**
- Proyecto de MaestrÃ­a en Ciencias de Datos
- Junio 2025

## ğŸ“š Referencias

- **Scikit-learn Documentation**: [https://scikit-learn.org/](https://scikit-learn.org/)
- **SDSS Survey**: [https://www.sdss.org/](https://www.sdss.org/)
- **Kaggle Dataset**: [CelestialClassify](https://www.kaggle.com/datasets/hari31416/celestialclassify)

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

---

â­ **Si este proyecto te resultÃ³ Ãºtil, Â¡no olvides darle una estrella!** â­ 