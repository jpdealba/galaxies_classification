\babel@toc {spanish}{}\relax 
\contentsline {figure}{\numberline {1}{\ignorespaces Información general del dataset obtenida con df.info(). Se confirma la estructura completa con 4,000,000 entradas, 51 columnas sin valores nulos, y un uso de memoria de 1.5+ GB. Los tipos de datos incluyen enteros (int64), flotantes (float64) y objetos (object) para la variable objetivo.}}{11}{figure.1}%
\contentsline {figure}{\numberline {2}{\ignorespaces Estadísticas descriptivas - Ejemplo: Variables de identificación, posición y magnitudes PSF. Se observa que todas las variables tienen el conteo completo de 4,000,000 observaciones, confirmando la ausencia de valores faltantes.}}{12}{figure.2}%
\contentsline {figure}{\numberline {3}{\ignorespaces Distribución de las clases en el dataset. Se observa que las variables tienen el conteo completo de 4,000,000 observaciones.}}{12}{figure.3}%
\contentsline {figure}{\numberline {4}{\ignorespaces Histogramas con curvas de densidad KDE de variables con distribuciones relativamente normales. Las curvas de densidad (generadas con \texttt {sns.histplot} y \texttt {kde=True}) proporcionan una representación suave de la distribución, facilitando la identificación de patrones y modas. Se muestran ejemplos representativos de las 52 columnas del dataset.}}{13}{figure.4}%
\contentsline {figure}{\numberline {5}{\ignorespaces Histogramas con curvas de densidad KDE de variables con distribuciones altamente sesgadas. Las curvas de densidad revelan claramente la concentración de valores cerca de cero y las colas extremas. La estimación por kernel (KDE) ayuda a visualizar mejor la forma de estas distribuciones asimétricas. Debido a la cantidad de columnas (52), se muestran ejemplos representativos.}}{14}{figure.5}%
\contentsline {figure}{\numberline {6}{\ignorespaces Boxplots de magnitudes fotométricas mostrando la presencia de valores atípicos. Estos outliers representan objetos astronómicos reales (muy brillantes o muy débiles) y contienen información valiosa para la clasificación.}}{15}{figure.6}%
\contentsline {figure}{\numberline {7}{\ignorespaces Boxplots de parámetros de Stokes y variables de movimiento. Se observa la concentración extrema de valores cerca de cero y la presencia de outliers exagerados, especialmente en los parámetros q\_* y u\_*.}}{15}{figure.7}%
\contentsline {figure}{\numberline {8}{\ignorespaces Boxplots de parámetros morfológicos (relaciones de ejes y algunos histogramas adicionales). Las variables expAB\_* muestran distribuciones más controladas, mientras que otras variables presentan comportamientos diversos. Debido a las 52 columnas del dataset, se presentan ejemplos representativos.}}{15}{figure.8}%
\contentsline {figure}{\numberline {9}{\ignorespaces Correlación de todas las variables con la variable objetivo (type\_numeric). Las variables con correlaciones más altas (en valor absoluto) son las más discriminativas para la clasificación.}}{16}{figure.9}%
\contentsline {figure}{\numberline {10}{\ignorespaces Matriz de correlación entre todas las variables del dataset. Los colores más intensos indican correlaciones más fuertes (positivas en azul, negativas en amarillo). Se observan bloques de alta correlación entre variables del mismo tipo (e.g., magnitudes en diferentes filtros, radios en diferentes bandas).}}{17}{figure.10}%
\contentsline {figure}{\numberline {11}{\ignorespaces Estadísticos chi-cuadrado para las variables seleccionadas. Valores más altos indican mayor asociación con la variable objetivo. Todas las variables muestran estadísticos muy elevados, confirmando su relevancia discriminativa.}}{20}{figure.11}%
\contentsline {figure}{\numberline {12}{\ignorespaces P-valores asociados a la prueba chi-cuadrado para cada variable. Todos los p-valores son prácticamente cero, rechazando la hipótesis nula de independencia.}}{21}{figure.12}%
\contentsline {figure}{\numberline {13}{\ignorespaces Diagrama del pipeline de preprocesado implementado. Se muestra la arquitectura del ColumnTransformer con dos ramas de procesamiento: una para variables que requieren transformación logarítmica y otra para variables estándar.}}{22}{figure.13}%
\contentsline {figure}{\numberline {14}{\ignorespaces Ejemplo de la efectividad de la transformación logarítmica en variables con distribuciones altamente sesgadas. Se muestra la comparación entre las distribuciones originales (izquierda) y después de aplicar log1p (derecha) para modelFlux\_i y modelFlux\_z. La transformación convierte distribuciones extremadamente sesgadas en distribuciones más simétricas y manejables para los algoritmos de machine learning.}}{23}{figure.14}%
\contentsline {figure}{\numberline {15}{\ignorespaces Pipeline completo para Regresión Logística, integrando el preprocesado especializado para datos astronómicos con el clasificador optimizado.}}{25}{figure.15}%
\contentsline {figure}{\numberline {16}{\ignorespaces Comparación entre train\_test\_split (80\%-20\%) y validación cruzada de 5 folds. Ambas técnicas muestran resultados consistentes, confirmando la estabilidad del modelo.}}{27}{figure.16}%
\contentsline {figure}{\numberline {17}{\ignorespaces Matriz de confusión del modelo final en el dataset de test. Se muestran las predicciones para 1 millón de objetos astronómicos clasificados como STAR (0) o GALAXY (1).}}{29}{figure.17}%
\contentsline {figure}{\numberline {18}{\ignorespaces Resultado del índice kappa de Cohen para el modelo final. El valor obtenido demuestra una concordancia casi perfecta entre las predicciones del modelo y las etiquetas verdaderas.}}{29}{figure.18}%
\contentsline {figure}{\numberline {19}{\ignorespaces Curva ROC del modelo final mostrando un AUC de 0.99. La curva se aproxima al punto óptimo (0,1), indicando un rendimiento excelente. La línea punteada representa el rendimiento aleatorio (AUC = 0.50).}}{30}{figure.19}%
